{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'Weather Training Data.csv'\n",
    "# Loading the dataset again to work with the original 'Location' values (string names)\n",
    "weather_data = pd.read_csv(file_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row ID</th>\n",
       "      <th>Location</th>\n",
       "      <th>MinTemp</th>\n",
       "      <th>MaxTemp</th>\n",
       "      <th>Rainfall</th>\n",
       "      <th>WindGustDir</th>\n",
       "      <th>WindGustSpeed</th>\n",
       "      <th>WindDir9am</th>\n",
       "      <th>WindDir3pm</th>\n",
       "      <th>WindSpeed9am</th>\n",
       "      <th>WindSpeed3pm</th>\n",
       "      <th>Humidity9am</th>\n",
       "      <th>Humidity3pm</th>\n",
       "      <th>Pressure9am</th>\n",
       "      <th>Pressure3pm</th>\n",
       "      <th>Temp9am</th>\n",
       "      <th>Temp3pm</th>\n",
       "      <th>RainToday</th>\n",
       "      <th>RainTomorrow</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6273</th>\n",
       "      <td>59784</td>\n",
       "      <td>11</td>\n",
       "      <td>16.1</td>\n",
       "      <td>31.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>54.0</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>7.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>1005.9</td>\n",
       "      <td>1002.3</td>\n",
       "      <td>26.5</td>\n",
       "      <td>28.4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6274</th>\n",
       "      <td>59795</td>\n",
       "      <td>11</td>\n",
       "      <td>22.8</td>\n",
       "      <td>24.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11</td>\n",
       "      <td>56.0</td>\n",
       "      <td>11</td>\n",
       "      <td>8</td>\n",
       "      <td>35.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>1010.9</td>\n",
       "      <td>1011.4</td>\n",
       "      <td>23.4</td>\n",
       "      <td>24.4</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6275</th>\n",
       "      <td>59805</td>\n",
       "      <td>11</td>\n",
       "      <td>14.8</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>7.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>1019.5</td>\n",
       "      <td>1017.0</td>\n",
       "      <td>22.5</td>\n",
       "      <td>24.8</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6276</th>\n",
       "      <td>59816</td>\n",
       "      <td>11</td>\n",
       "      <td>15.5</td>\n",
       "      <td>27.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>41.0</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>7.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>1015.7</td>\n",
       "      <td>1012.7</td>\n",
       "      <td>24.6</td>\n",
       "      <td>26.1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6277</th>\n",
       "      <td>59827</td>\n",
       "      <td>11</td>\n",
       "      <td>22.7</td>\n",
       "      <td>29.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11</td>\n",
       "      <td>44.0</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>9.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>1009.4</td>\n",
       "      <td>1008.8</td>\n",
       "      <td>25.8</td>\n",
       "      <td>26.4</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      row ID  Location  MinTemp  MaxTemp  Rainfall  WindGustDir   \n",
       "6273   59784        11     16.1     31.4       0.0            5  \\\n",
       "6274   59795        11     22.8     24.7       0.0           11   \n",
       "6275   59805        11     14.8     25.0       0.8            0   \n",
       "6276   59816        11     15.5     27.3       0.0            4   \n",
       "6277   59827        11     22.7     29.2       0.0           11   \n",
       "\n",
       "      WindGustSpeed  WindDir9am  WindDir3pm  WindSpeed9am  WindSpeed3pm   \n",
       "6273           54.0           3           5           7.0          37.0  \\\n",
       "6274           56.0          11           8          35.0          15.0   \n",
       "6275           24.0          10           2           7.0          17.0   \n",
       "6276           41.0           7           4           7.0          30.0   \n",
       "6277           44.0           7           5           9.0          20.0   \n",
       "\n",
       "      Humidity9am  Humidity3pm  Pressure9am  Pressure3pm  Temp9am  Temp3pm   \n",
       "6273         51.0         58.0       1005.9       1002.3     26.5     28.4  \\\n",
       "6274         68.0         67.0       1010.9       1011.4     23.4     24.4   \n",
       "6275         62.0         45.0       1019.5       1017.0     22.5     24.8   \n",
       "6276         54.0         62.0       1015.7       1012.7     24.6     26.1   \n",
       "6277         70.0         71.0       1009.4       1008.8     25.8     26.4   \n",
       "\n",
       "      RainToday  RainTomorrow  \n",
       "6273          0           0.0  \n",
       "6274          0           1.0  \n",
       "6275          0           0.0  \n",
       "6276          0           0.0  \n",
       "6277          0           1.0  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "import numpy as np\n",
    "\n",
    "# Preprocessing steps for the whole dataset\n",
    "\n",
    "# # Dropping 'row ID' as it's not needed for model training\n",
    "# weather_data = weather_data.drop(['row ID'], axis=1)\n",
    "\n",
    "# Handling missing values for numerical columns\n",
    "num_cols = weather_data.select_dtypes(include=[np.number]).columns\n",
    "imputer_num = SimpleImputer(strategy='median')\n",
    "weather_data[num_cols] = imputer_num.fit_transform(weather_data[num_cols])\n",
    "\n",
    "# Handling missing values for categorical columns\n",
    "# For now, we will drop columns with a high percentage of missing values\n",
    "weather_data = weather_data.drop(['Evaporation', 'Sunshine', 'Cloud9am', 'Cloud3pm'], axis=1)\n",
    "cat_cols = weather_data.select_dtypes(include=['object']).columns\n",
    "imputer_cat = SimpleImputer(strategy='most_frequent')\n",
    "weather_data[cat_cols] = imputer_cat.fit_transform(weather_data[cat_cols])\n",
    "\n",
    "\n",
    "\n",
    "# Encoding categorical variables\n",
    "label_encoders = {}\n",
    "for column in cat_cols:\n",
    "    label_encoder = LabelEncoder()\n",
    "    weather_data[column] = label_encoder.fit_transform(weather_data[column].astype(str))\n",
    "    label_encoders[column] = label_encoder\n",
    "\n",
    "# Adjusting the provided code to extract data for all coastal cities in Australia\n",
    "\n",
    "# First, let's define the coastal cities as per the original dataset\n",
    "coastal_cities = ['Adelaide', 'Albany', 'Brisbane', 'Cairns', 'CoffsHarbour', \n",
    "                  'Darwin', 'GoldCoast', 'Hobart', 'Melbourne', 'MelbourneAirport',\n",
    "                  'Newcastle', 'NorahHead', 'NorfolkIsland', 'Perth', 'PerthAirport',\n",
    "                  'Portland', 'Sydney', 'SydneyAirport', 'Townsville', 'Williamtown', 'Wollongong']\n",
    "\n",
    "# Now, let's extract the encoded values for these coastal cities\n",
    "encoded_coastal_cities = {label_encoders['Location'].transform([city])[0] for city in coastal_cities if city in label_encoders['Location'].classes_}\n",
    "\n",
    "# Extracting data for these coastal cities\n",
    "coastal_data = weather_data[weather_data['Location'].isin(encoded_coastal_cities)]\n",
    "\n",
    "# Overview of the processed coastal cities data\n",
    "coastal_data.head()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping the encoded integer values back to their original string labels in 'Location' column\n",
    "location_mappings = {index: label for index, label in enumerate(label_encoders['Location'].classes_)}\n",
    "# location_mappings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Training set size': 30885,\n",
       " 'Validation set size': 6618,\n",
       " 'Test set size': 6619}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Splitting the dataset into features (X) and target variable (y)\n",
    "X = coastal_data.drop(columns=['RainTomorrow'])\n",
    "y = coastal_data['RainTomorrow']\n",
    "\n",
    "# Splitting the data into training, validation, and test sets\n",
    "# Using 70% of data for training, 15% for validation, and 15% for testing\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "y_train = y_train.astype('int')\n",
    "y_val = y_val.astype('int')\n",
    "y_test = y_test.astype('int')\n",
    "\n",
    "# Sizes of each dataset\n",
    "sizes = {\n",
    "    \"Training set size\": X_train.shape[0],\n",
    "    \"Validation set size\": X_val.shape[0],\n",
    "    \"Test set size\": X_test.shape[0]\n",
    "}\n",
    "\n",
    "sizes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters from Grid Search: {'C': 0.1, 'penalty': 'l1'}\n",
      "Best Score from Grid Search on Validation Set: 0.8136900480232375\n",
      "Accuracy of Optimized Model on Test Set: 0.8116029611723825\n",
      "Classification Report for Test Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.94      0.88      4860\n",
      "           1       0.72      0.47      0.57      1759\n",
      "\n",
      "    accuracy                           0.81      6619\n",
      "   macro avg       0.78      0.70      0.72      6619\n",
      "weighted avg       0.80      0.81      0.80      6619\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Initialize Logistic Regression model\n",
    "logreg = LogisticRegression(solver='liblinear', random_state=42)\n",
    "\n",
    "# Parameters for tuning\n",
    "param_grid = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100], # Regularization parameter\n",
    "    'penalty': ['l1', 'l2'] # Norm used in the penalization\n",
    "}\n",
    "\n",
    "# Grid search with cross-validation for parameter tuning\n",
    "grid_search = GridSearchCV(logreg, param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_val, y_val)\n",
    "\n",
    "# Best parameters and best score\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "# Train the model with the best parameters on the full training set\n",
    "logreg_optimized = LogisticRegression(**best_params, solver='liblinear', random_state=42)\n",
    "logreg_optimized.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = logreg_optimized.predict(X_test)\n",
    "\n",
    "# Evaluate the optimized model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "classification_rep = classification_report(y_test, y_pred)\n",
    "\n",
    "# Printing the best parameters found by Grid Search\n",
    "print(\"Best Parameters from Grid Search:\", best_params)\n",
    "\n",
    "# Printing the best score achieved on the validation set during Grid Search\n",
    "print(\"Best Score from Grid Search on Validation Set:\", best_score)\n",
    "\n",
    "# Printing the accuracy of the optimized model on the test set\n",
    "print(\"Accuracy of Optimized Model on Test Set:\", accuracy)\n",
    "\n",
    "# Printing the detailed classification report for the test set predictions\n",
    "print(\"Classification Report for Test Set:\")\n",
    "print(classification_rep)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters from Grid Search (Decision Tree): {'max_depth': 10, 'min_samples_leaf': 2, 'min_samples_split': 2}\n",
      "Best Score from Grid Search on Validation Set (Decision Tree): 0.7796931125588873\n",
      "Accuracy of Optimized Decision Tree Model on Test Set: 0.7995165432844841\n",
      "Classification Report for Test Set (Decision Tree):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.92      0.87      4860\n",
      "           1       0.68      0.47      0.56      1759\n",
      "\n",
      "    accuracy                           0.80      6619\n",
      "   macro avg       0.75      0.69      0.71      6619\n",
      "weighted avg       0.79      0.80      0.79      6619\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Initialize Decision Tree Classifier\n",
    "dtree = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Parameters for tuning\n",
    "param_grid_dtree = {\n",
    "    'max_depth': [None, 10, 20, 30, 40, 50],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Grid search with cross-validation for parameter tuning\n",
    "grid_search_dtree = GridSearchCV(dtree, param_grid_dtree, cv=5, scoring='accuracy')\n",
    "grid_search_dtree.fit(X_val, y_val)\n",
    "\n",
    "# Best parameters and best score\n",
    "best_params_dtree = grid_search_dtree.best_params_\n",
    "best_score_dtree = grid_search_dtree.best_score_\n",
    "\n",
    "# Train the model with the best parameters on the full training set\n",
    "dtree_optimized = DecisionTreeClassifier(**best_params_dtree, random_state=42)\n",
    "dtree_optimized.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_dtree = dtree_optimized.predict(X_test)\n",
    "\n",
    "# Evaluate the optimized model\n",
    "accuracy_dtree = accuracy_score(y_test, y_pred_dtree)\n",
    "classification_rep_dtree = classification_report(y_test, y_pred_dtree)\n",
    "\n",
    "# Printing the best parameters found by Grid Search\n",
    "print(\"Best Parameters from Grid Search (Decision Tree):\", best_params_dtree)\n",
    "\n",
    "# Printing the best score achieved on the validation set during Grid Search\n",
    "print(\"Best Score from Grid Search on Validation Set (Decision Tree):\", best_score_dtree)\n",
    "\n",
    "# Printing the accuracy of the optimized model on the test set\n",
    "print(\"Accuracy of Optimized Decision Tree Model on Test Set:\", accuracy_dtree)\n",
    "\n",
    "# Printing the detailed classification report for the test set predictions\n",
    "print(\"Classification Report for Test Set (Decision Tree):\")\n",
    "print(classification_rep_dtree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters from Grid Search (Decision Tree): {'max_depth': None, 'min_samples_leaf': 4, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "Best Score from Grid Search on Validation Set (Decision Tree): 0.8159555665166367\n",
      "Accuracy of Optimized Decision Tree Model on Test Set: 0.8259555824142619\n",
      "Classification Report for Test Set (Decision Tree):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.94      0.89      4860\n",
      "           1       0.76      0.51      0.61      1759\n",
      "\n",
      "    accuracy                           0.83      6619\n",
      "   macro avg       0.80      0.72      0.75      6619\n",
      "weighted avg       0.82      0.83      0.81      6619\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Initialize Random Forest Classifier\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Parameters for tuning\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Grid search with cross-validation for parameter tuning\n",
    "grid_search_rf = GridSearchCV(rf, param_grid_rf, cv=5, scoring='accuracy')\n",
    "grid_search_rf.fit(X_val, y_val)\n",
    "\n",
    "# Best parameters and best score\n",
    "best_params_rf = grid_search_rf.best_params_\n",
    "best_score_rf = grid_search_rf.best_score_\n",
    "\n",
    "# Train the model with the best parameters on the full training set\n",
    "rf_optimized = RandomForestClassifier(**best_params_rf, random_state=42)\n",
    "rf_optimized.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_rf = rf_optimized.predict(X_test)\n",
    "\n",
    "# Evaluate the optimized model\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "classification_rep_rf = classification_report(y_test, y_pred_rf)\n",
    "\n",
    "# Printing the best parameters found by Grid Search\n",
    "print(\"Best Parameters from Grid Search (Decision Tree):\", best_params_rf)\n",
    "\n",
    "# Printing the best score achieved on the validation set during Grid Search\n",
    "print(\"Best Score from Grid Search on Validation Set (Decision Tree):\", best_score_rf)\n",
    "\n",
    "# Printing the accuracy of the optimized model on the test set\n",
    "print(\"Accuracy of Optimized Decision Tree Model on Test Set:\", accuracy_rf)\n",
    "\n",
    "# Printing the detailed classification report for the test set predictions\n",
    "print(\"Classification Report for Test Set (Decision Tree):\")\n",
    "print(classification_rep_rf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "966/966 [==============================] - 1s 795us/step - loss: 0.4538 - accuracy: 0.7961 - val_loss: 0.4229 - val_accuracy: 0.8146\n",
      "Epoch 2/50\n",
      "966/966 [==============================] - 1s 691us/step - loss: 0.4251 - accuracy: 0.8133 - val_loss: 0.4174 - val_accuracy: 0.8184\n",
      "Epoch 3/50\n",
      "966/966 [==============================] - 1s 686us/step - loss: 0.4185 - accuracy: 0.8177 - val_loss: 0.4154 - val_accuracy: 0.8199\n",
      "Epoch 4/50\n",
      "966/966 [==============================] - 1s 688us/step - loss: 0.4140 - accuracy: 0.8193 - val_loss: 0.4101 - val_accuracy: 0.8191\n",
      "Epoch 5/50\n",
      "966/966 [==============================] - 1s 681us/step - loss: 0.4113 - accuracy: 0.8205 - val_loss: 0.4082 - val_accuracy: 0.8200\n",
      "Epoch 6/50\n",
      "966/966 [==============================] - 1s 676us/step - loss: 0.4106 - accuracy: 0.8204 - val_loss: 0.4092 - val_accuracy: 0.8199\n",
      "Epoch 7/50\n",
      "966/966 [==============================] - 1s 696us/step - loss: 0.4081 - accuracy: 0.8188 - val_loss: 0.4070 - val_accuracy: 0.8184\n",
      "Epoch 8/50\n",
      "966/966 [==============================] - 1s 686us/step - loss: 0.4074 - accuracy: 0.8215 - val_loss: 0.4060 - val_accuracy: 0.8196\n",
      "Epoch 9/50\n",
      "966/966 [==============================] - 1s 692us/step - loss: 0.4066 - accuracy: 0.8222 - val_loss: 0.4046 - val_accuracy: 0.8211\n",
      "Epoch 10/50\n",
      "966/966 [==============================] - 1s 688us/step - loss: 0.4065 - accuracy: 0.8221 - val_loss: 0.4036 - val_accuracy: 0.8209\n",
      "Epoch 11/50\n",
      "966/966 [==============================] - 1s 697us/step - loss: 0.4047 - accuracy: 0.8235 - val_loss: 0.4033 - val_accuracy: 0.8214\n",
      "Epoch 12/50\n",
      "966/966 [==============================] - 1s 690us/step - loss: 0.4037 - accuracy: 0.8231 - val_loss: 0.4016 - val_accuracy: 0.8193\n",
      "Epoch 13/50\n",
      "966/966 [==============================] - 1s 781us/step - loss: 0.4033 - accuracy: 0.8235 - val_loss: 0.4021 - val_accuracy: 0.8205\n",
      "Epoch 14/50\n",
      "966/966 [==============================] - 1s 740us/step - loss: 0.4031 - accuracy: 0.8254 - val_loss: 0.4020 - val_accuracy: 0.8209\n",
      "Epoch 15/50\n",
      "966/966 [==============================] - 1s 681us/step - loss: 0.4005 - accuracy: 0.8255 - val_loss: 0.4020 - val_accuracy: 0.8212\n",
      "Epoch 16/50\n",
      "966/966 [==============================] - 1s 688us/step - loss: 0.4019 - accuracy: 0.8249 - val_loss: 0.4030 - val_accuracy: 0.8190\n",
      "Epoch 17/50\n",
      "966/966 [==============================] - 1s 714us/step - loss: 0.4020 - accuracy: 0.8247 - val_loss: 0.4011 - val_accuracy: 0.8191\n",
      "Epoch 18/50\n",
      "966/966 [==============================] - 1s 704us/step - loss: 0.3986 - accuracy: 0.8256 - val_loss: 0.4010 - val_accuracy: 0.8190\n",
      "Epoch 19/50\n",
      "966/966 [==============================] - 1s 688us/step - loss: 0.3995 - accuracy: 0.8257 - val_loss: 0.4002 - val_accuracy: 0.8205\n",
      "Epoch 20/50\n",
      "966/966 [==============================] - 1s 697us/step - loss: 0.3987 - accuracy: 0.8257 - val_loss: 0.4009 - val_accuracy: 0.8199\n",
      "Epoch 21/50\n",
      "966/966 [==============================] - 1s 679us/step - loss: 0.3984 - accuracy: 0.8259 - val_loss: 0.4008 - val_accuracy: 0.8185\n",
      "Epoch 22/50\n",
      "966/966 [==============================] - 1s 681us/step - loss: 0.3995 - accuracy: 0.8262 - val_loss: 0.4010 - val_accuracy: 0.8211\n",
      "Epoch 23/50\n",
      "966/966 [==============================] - 1s 684us/step - loss: 0.3982 - accuracy: 0.8253 - val_loss: 0.3986 - val_accuracy: 0.8217\n",
      "Epoch 24/50\n",
      "966/966 [==============================] - 1s 686us/step - loss: 0.3962 - accuracy: 0.8287 - val_loss: 0.3993 - val_accuracy: 0.8193\n",
      "Epoch 25/50\n",
      "966/966 [==============================] - 1s 695us/step - loss: 0.3972 - accuracy: 0.8261 - val_loss: 0.3962 - val_accuracy: 0.8212\n",
      "Epoch 26/50\n",
      "966/966 [==============================] - 1s 685us/step - loss: 0.3959 - accuracy: 0.8265 - val_loss: 0.3984 - val_accuracy: 0.8217\n",
      "Epoch 27/50\n",
      "966/966 [==============================] - 1s 680us/step - loss: 0.3962 - accuracy: 0.8267 - val_loss: 0.3981 - val_accuracy: 0.8215\n",
      "Epoch 28/50\n",
      "966/966 [==============================] - 1s 721us/step - loss: 0.3959 - accuracy: 0.8276 - val_loss: 0.3993 - val_accuracy: 0.8209\n",
      "Epoch 29/50\n",
      "966/966 [==============================] - 1s 706us/step - loss: 0.3936 - accuracy: 0.8283 - val_loss: 0.3974 - val_accuracy: 0.8229\n",
      "Epoch 30/50\n",
      "966/966 [==============================] - 1s 686us/step - loss: 0.3934 - accuracy: 0.8299 - val_loss: 0.3970 - val_accuracy: 0.8212\n",
      "Epoch 31/50\n",
      "966/966 [==============================] - 1s 680us/step - loss: 0.3950 - accuracy: 0.8278 - val_loss: 0.3989 - val_accuracy: 0.8209\n",
      "Epoch 32/50\n",
      "966/966 [==============================] - 1s 679us/step - loss: 0.3927 - accuracy: 0.8289 - val_loss: 0.3959 - val_accuracy: 0.8228\n",
      "Epoch 33/50\n",
      "966/966 [==============================] - 1s 686us/step - loss: 0.3924 - accuracy: 0.8297 - val_loss: 0.3974 - val_accuracy: 0.8212\n",
      "Epoch 34/50\n",
      "966/966 [==============================] - 1s 681us/step - loss: 0.3939 - accuracy: 0.8291 - val_loss: 0.3953 - val_accuracy: 0.8235\n",
      "Epoch 35/50\n",
      "966/966 [==============================] - 1s 679us/step - loss: 0.3921 - accuracy: 0.8278 - val_loss: 0.3973 - val_accuracy: 0.8229\n",
      "Epoch 36/50\n",
      "966/966 [==============================] - 1s 677us/step - loss: 0.3922 - accuracy: 0.8283 - val_loss: 0.3969 - val_accuracy: 0.8208\n",
      "Epoch 37/50\n",
      "966/966 [==============================] - 1s 696us/step - loss: 0.3914 - accuracy: 0.8288 - val_loss: 0.3986 - val_accuracy: 0.8199\n",
      "Epoch 38/50\n",
      "966/966 [==============================] - 1s 676us/step - loss: 0.3919 - accuracy: 0.8288 - val_loss: 0.3981 - val_accuracy: 0.8226\n",
      "Epoch 39/50\n",
      "966/966 [==============================] - 1s 711us/step - loss: 0.3934 - accuracy: 0.8292 - val_loss: 0.3956 - val_accuracy: 0.8197\n",
      "Epoch 40/50\n",
      "966/966 [==============================] - 1s 682us/step - loss: 0.3929 - accuracy: 0.8291 - val_loss: 0.3949 - val_accuracy: 0.8225\n",
      "Epoch 41/50\n",
      "966/966 [==============================] - 1s 691us/step - loss: 0.3927 - accuracy: 0.8295 - val_loss: 0.3963 - val_accuracy: 0.8226\n",
      "Epoch 42/50\n",
      "966/966 [==============================] - 1s 682us/step - loss: 0.3895 - accuracy: 0.8293 - val_loss: 0.3943 - val_accuracy: 0.8231\n",
      "Epoch 43/50\n",
      "966/966 [==============================] - 1s 693us/step - loss: 0.3908 - accuracy: 0.8288 - val_loss: 0.3951 - val_accuracy: 0.8205\n",
      "Epoch 44/50\n",
      "966/966 [==============================] - 1s 694us/step - loss: 0.3919 - accuracy: 0.8301 - val_loss: 0.3932 - val_accuracy: 0.8218\n",
      "Epoch 45/50\n",
      "966/966 [==============================] - 1s 692us/step - loss: 0.3896 - accuracy: 0.8300 - val_loss: 0.3945 - val_accuracy: 0.8231\n",
      "Epoch 46/50\n",
      "966/966 [==============================] - 1s 696us/step - loss: 0.3902 - accuracy: 0.8297 - val_loss: 0.3943 - val_accuracy: 0.8223\n",
      "Epoch 47/50\n",
      "966/966 [==============================] - 1s 683us/step - loss: 0.3899 - accuracy: 0.8310 - val_loss: 0.3944 - val_accuracy: 0.8232\n",
      "Epoch 48/50\n",
      "966/966 [==============================] - 1s 680us/step - loss: 0.3906 - accuracy: 0.8301 - val_loss: 0.3945 - val_accuracy: 0.8238\n",
      "Epoch 49/50\n",
      "966/966 [==============================] - 1s 707us/step - loss: 0.3910 - accuracy: 0.8294 - val_loss: 0.3957 - val_accuracy: 0.8214\n",
      "Epoch 50/50\n",
      "966/966 [==============================] - 1s 732us/step - loss: 0.3901 - accuracy: 0.8305 - val_loss: 0.3965 - val_accuracy: 0.8237\n",
      "207/207 [==============================] - 0s 371us/step - loss: 0.3989 - accuracy: 0.8243\n",
      "207/207 [==============================] - 0s 324us/step\n",
      "Test Accuracy: 0.8242936730384827\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.97      0.89      4860\n",
      "         1.0       0.83      0.43      0.57      1759\n",
      "\n",
      "    accuracy                           0.82      6619\n",
      "   macro avg       0.82      0.70      0.73      6619\n",
      "weighted avg       0.82      0.82      0.80      6619\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Assuming coastal_data is your preprocessed dataset\n",
    "X = coastal_data.drop(['RainTomorrow'], axis=1)\n",
    "y = coastal_data['RainTomorrow']\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Normalize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Building the revised model\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_shape=(X_train_scaled.shape[1],), activation='relu'))\n",
    "model.add(Dropout(0.5))  # Dropout layer for regularization\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))  # Another dropout layer\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compiling the model with a different optimizer and learning rate\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Early stopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Training the model with more epochs and early stopping\n",
    "history = model.fit(X_train_scaled, y_train, epochs=50, batch_size=32, \n",
    "                    validation_data=(X_val_scaled, y_val), callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate the revised model\n",
    "loss, accuracy = model.evaluate(X_test_scaled, y_test)\n",
    "\n",
    "# Predictions on the test set\n",
    "y_pred_probs = model.predict(X_test_scaled)\n",
    "y_pred_classes = (y_pred_probs > 0.5).astype(int)\n",
    "\n",
    "# Compute classification report\n",
    "classification_rep = classification_report(y_test, y_pred_classes)\n",
    "\n",
    "# Print the accuracy and the detailed classification report\n",
    "print(\"Test Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_rep)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
